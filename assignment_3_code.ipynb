{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5682f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Gensim for corpus and model\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Import NLTK for stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477ddef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the BBC news dataset\n",
    "data_folder=\".../bbc\"\n",
    "\n",
    "folders=[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"]\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "\n",
    "for i in folders:\n",
    "    files=os.listdir(data_folder+'/'+i)\n",
    "    for text_file in files:\n",
    "        file_path=data_folder + '/'+i+'/'+text_file\n",
    "        with open(file_path,'rb') as f:\n",
    "            data=f.read()\n",
    "        x.append(data)\n",
    "        y.append(i)\n",
    "        \n",
    "data={'text':x,'type':y}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e811db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gensim's simple preprocess to tokenize words from sentences\n",
    "def tokenize(sentences, deacc=True):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650b7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords from NLTK's model of english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Def function for removing stopwords in each tokenized text body\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca497ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data and tokenize body texts\n",
    "data = df['text'].values.tolist()\n",
    "data_words = list(tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec9a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stopwords from tokenized texts\n",
    "data_no_stopwords = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de37a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29656"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build dictionary from POS tagged data\n",
    "dictionary = corpora.Dictionary(data_no_stopwords)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277ee73",
   "metadata": {},
   "source": [
    "*Note*\n",
    "\n",
    "There should be 27,676 words indexed using the gensim corpora. However, I am getting 29,656 indexed words in the dictionary after removing stopwords. When using lemmatization used in the guide, this reduces to 18,241 indexed words. this might be due to updates with the nltk's model of english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8930503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus using lemmatized texts\n",
    "texts = data_no_stopwords\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9c9a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather labels as a list for passing through train-test split\n",
    "labels = df['type'].values.tolist()\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd609a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662ca603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using train-test split\n",
    "LDA_model = gensim.models.ldamodel.LdaModel(corpus=X_train,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha=\"auto\")\n",
    "                                            #per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3413b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4573562264949323\n"
     ]
    }
   ],
   "source": [
    "# Get coherence score\n",
    "coherence_model_lda = CoherenceModel(model=LDA_model, texts=data_no_stopwords, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
