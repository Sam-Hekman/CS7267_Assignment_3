{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cbb63c",
   "metadata": {},
   "source": [
    "## Evaluation of base model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5682f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Gensim for corpus and model\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Import NLTK for stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477ddef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the BBC news dataset\n",
    "data_folder=\".../bbc\"\n",
    "\n",
    "folders=[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"]\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "\n",
    "for i in folders:\n",
    "    files=os.listdir(data_folder+'/'+i)\n",
    "    for text_file in files:\n",
    "        file_path=data_folder + '/'+i+'/'+text_file\n",
    "        with open(file_path,'rb') as f:\n",
    "            data=f.read()\n",
    "        x.append(data)\n",
    "        y.append(i)\n",
    "        \n",
    "data={'text':x,'type':y}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e811db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gensim's simple preprocess to tokenize words from sentences\n",
    "def tokenize(sentences, deacc=True):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650b7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords from NLTK's model of english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Def function for removing stopwords in each tokenized text body\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec9a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data and tokenize body texts\n",
    "data = df['text'].values.tolist()\n",
    "data_words = list(tokenize(data))\n",
    "\n",
    "# Filter stopwords from tokenized texts\n",
    "data_no_stopwords = remove_stopwords(data_words)\n",
    "\n",
    "# Build dictionary from POS tagged data\n",
    "dictionary = corpora.Dictionary(data_no_stopwords)\n",
    "\n",
    "# Build corpus using lemmatized texts\n",
    "texts = data_no_stopwords\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Gather labels as a list for passing through train-test split\n",
    "labels = df['type'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd609a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662ca603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using train-test split\n",
    "LDA_model = gensim.models.ldamodel.LdaModel(corpus=X_train,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=5,\n",
    "                                            random_state=42,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha=\"auto\")\n",
    "                                            #per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81e25b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"said\" + 0.010*\"us\" + 0.010*\"bn\" + 0.010*\"year\" + 0.009*\"xc\" + 0.009*\"xa\" + 0.006*\"company\" + 0.006*\"market\" + 0.005*\"nthe\" + 0.005*\"growth\"'),\n",
       " (1,\n",
       "  '0.022*\"said\" + 0.019*\"mr\" + 0.013*\"would\" + 0.010*\"government\" + 0.009*\"labour\" + 0.007*\"election\" + 0.007*\"party\" + 0.006*\"blair\" + 0.005*\"people\" + 0.005*\"minister\"'),\n",
       " (2,\n",
       "  '0.015*\"said\" + 0.009*\"people\" + 0.005*\"would\" + 0.005*\"new\" + 0.005*\"mr\" + 0.005*\"also\" + 0.005*\"could\" + 0.004*\"one\" + 0.004*\"nthe\" + 0.004*\"technology\"'),\n",
       " (3,\n",
       "  '0.009*\"said\" + 0.006*\"game\" + 0.006*\"england\" + 0.006*\"first\" + 0.005*\"year\" + 0.005*\"win\" + 0.005*\"time\" + 0.005*\"two\" + 0.004*\"last\" + 0.004*\"one\"'),\n",
       " (4,\n",
       "  '0.012*\"film\" + 0.010*\"best\" + 0.007*\"said\" + 0.007*\"year\" + 0.006*\"one\" + 0.006*\"also\" + 0.005*\"show\" + 0.005*\"music\" + 0.004*\"us\" + 0.004*\"awards\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61768c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for adding each possible unique pair of testing texts or labels\n",
    "def nCk_pairs(array):\n",
    "    paired = []\n",
    "    \n",
    "    for n in range(len(array)):\n",
    "        for k in range(len(array[n:])):\n",
    "            if k != 0:\n",
    "                paired.append([array[n],array[n+k]])\n",
    "    return paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6dfc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2d arrays for pairs of x_test and y_test\n",
    "testing_labels = nCk_pairs(y_test)\n",
    "testing_texts = nCk_pairs(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa5889ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98790\n",
      "98790\n"
     ]
    }
   ],
   "source": [
    "# Check that both are the same size array (445 choose 2)\n",
    "print(len(testing_texts))\n",
    "print(len(testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91fd048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting highest predicted cluster of text\n",
    "def predict_topic(array):\n",
    "    pred_topic = array[0][0]\n",
    "    best_similarity = array[0][1]\n",
    "    \n",
    "    for n in range(len(array)):\n",
    "        if array[n][1] > best_similarity:\n",
    "            pred_topic = array[n][0]\n",
    "            best_similarity = array[n][1]\n",
    "    return pred_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4dc844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paired list of predicted clusters, this cell takes a couple of minutes to get all predictions\n",
    "testing_results = []\n",
    "\n",
    "for text in range(len(testing_texts)):\n",
    "    a = predict_topic(LDA_model.get_document_topics(testing_texts[text][0]))\n",
    "    b = predict_topic(LDA_model.get_document_topics(testing_texts[text][1]))\n",
    "    testing_results.append([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d86cf73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ground truth and prediction \n",
    "def get_results(texts, labels):\n",
    "    # same GT, same pred\n",
    "    GS_PS = 0\n",
    "    # same GT, different pred\n",
    "    GS_PD = 0\n",
    "    # differnt GT, same pred\n",
    "    GD_PS = 0\n",
    "    # different GT, different pred\n",
    "    GD_PD = 0\n",
    "    \n",
    "    for n in range(len(texts)):\n",
    "        if texts[n][0] == texts[n][1] and labels[n][0] == labels[n][1]:\n",
    "            GS_PS += 1\n",
    "        elif texts[n][0] != texts[n][1] and labels[n][0] == labels[n][1]:\n",
    "            GS_PD += 1\n",
    "        elif texts[n][0] == texts[n][1] and labels[n][0] != labels[n][1]:\n",
    "            GD_PS += 1\n",
    "        elif texts[n][0] != texts[n][1] and labels[n][0] != labels[n][1]:\n",
    "            GD_PD += 1\n",
    "   \n",
    "    precision = GS_PS/(GS_PS+GD_PS)\n",
    "    recall = GS_PS/(GS_PS+GS_PD)\n",
    "    F1 = (2*precision*recall)/(precision+recall)\n",
    "    \n",
    "    print(\"--- Results -----------------\")\n",
    "    print(\"GT same, PRED same: {}\".format(GS_PS))\n",
    "    print(\"GT same, PRED different: {}\".format(GS_PD))\n",
    "    print(\"GT different, PRED same: {}\".format(GD_PS))\n",
    "    print(\"GT different, PRED different: {}\".format(GD_PD))\n",
    "    print(\"\\nPrecision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1 score: {}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "064efa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Results -----------------\n",
      "GT same, PRED same: 17418\n",
      "GT same, PRED different: 2854\n",
      "GT different, PRED same: 2675\n",
      "GT different, PRED different: 75843\n",
      "\n",
      "Precision: 0.8668690588762256\n",
      "Recall: 0.859214680347277\n",
      "F1 score: 0.8630248978075066\n"
     ]
    }
   ],
   "source": [
    "get_results(testing_results, testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c66f73",
   "metadata": {},
   "source": [
    "The prediction is substantially better than the prediction result in the assignment description. Additionally, the recall and overall F1 score are good as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
